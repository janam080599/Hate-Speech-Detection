{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(r\"C:\\Users\\akxjo\\Desktop\\SRI Work\\SRI_mycode\\train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1445</td>\n",
       "      <td>body%20bagging</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Body Bagging Bitches ???? http://t.co/aFssGPnZWi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>1447</td>\n",
       "      <td>body%20bagging</td>\n",
       "      <td>PURPLE BOOTH STUDIOã¢</td>\n",
       "      <td>No better feeling than seeing and being on sta...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1448</td>\n",
       "      <td>body%20bagging</td>\n",
       "      <td>Cloud 9</td>\n",
       "      <td>Mopheme and Bigstar Johnson are a problem in t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1449</td>\n",
       "      <td>body%20bagging</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I was body Bagging on the ????Today...4got to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1450</td>\n",
       "      <td>body%20bagging</td>\n",
       "      <td>Former Yugoslav Republic of Macedonia</td>\n",
       "      <td>@editaxohaze then let the bagging body's begin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id         keyword                               location  \\\n",
       "0       1             NaN                                    NaN   \n",
       "1       4             NaN                                    NaN   \n",
       "2       5             NaN                                    NaN   \n",
       "3       6             NaN                                    NaN   \n",
       "4       7             NaN                                    NaN   \n",
       "..    ...             ...                                    ...   \n",
       "995  1445  body%20bagging                                    NaN   \n",
       "996  1447  body%20bagging                 PURPLE BOOTH STUDIOã¢   \n",
       "997  1448  body%20bagging                                Cloud 9   \n",
       "998  1449  body%20bagging                                    NaN   \n",
       "999  1450  body%20bagging  Former Yugoslav Republic of Macedonia   \n",
       "\n",
       "                                                  text  target  \n",
       "0    Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1               Forest fire near La Ronge Sask. Canada       1  \n",
       "2    All residents asked to 'shelter in place' are ...       1  \n",
       "3    13,000 people receive #wildfires evacuation or...       1  \n",
       "4    Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "..                                                 ...     ...  \n",
       "995   Body Bagging Bitches ???? http://t.co/aFssGPnZWi       0  \n",
       "996  No better feeling than seeing and being on sta...       0  \n",
       "997  Mopheme and Bigstar Johnson are a problem in t...       0  \n",
       "998  I was body Bagging on the ????Today...4got to ...       0  \n",
       "999  @editaxohaze then let the bagging body's begin...       0  \n",
       "\n",
       "[1000 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['deed reason earthquak may allah forgiv us',\n",
       " 'forest fire near la rong sask canada',\n",
       " 'resid ask shelter place notifi offic evacu shelter place order expect',\n",
       " 'peopl receiv wildfir evacu order california',\n",
       " 'got sent photo rubi alaska smoke wildfir pour school',\n",
       " 'rockyfir updat california hwi close direct due lake counti fire cafir wildfir',\n",
       " 'flood disast heavi rain caus flash flood street manit colorado spring area',\n",
       " 'top hill see fire wood',\n",
       " 'emerg evacu happen build across street',\n",
       " 'afraid tornado come area',\n",
       " 'three peopl die heat wave far',\n",
       " 'haha south tampa get flood hah wait second live south tampa gon na gon na fvck flood',\n",
       " 'rain flood florida tampabay tampa day lost count',\n",
       " 'flood bago myanmar arriv bago',\n",
       " 'damag school bu multi car crash break',\n",
       " 'man',\n",
       " 'love fruit',\n",
       " 'summer love',\n",
       " 'car fast',\n",
       " 'goooooooaaaaaal',\n",
       " 'ridicul',\n",
       " 'london cool',\n",
       " 'love ski',\n",
       " 'wonder day',\n",
       " 'looooool',\n",
       " 'wayi not eat shit',\n",
       " 'nyc last week',\n",
       " 'love girlfriend',\n",
       " 'cooool',\n",
       " 'like pasta']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(df):\n",
    "    all_text=list()\n",
    "    lines=df[\"text\"].values.tolist()\n",
    "    for text in lines:\n",
    "        text=text.lower()\n",
    "        pattern=re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "        text = pattern.sub('', text)\n",
    "        emoji=re.compile(\"[\"\n",
    "                         u\"\\U0001F600-\\U0001FFFF\"  #emoticons\n",
    "                         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                         u\"\\U00002702-\\U000027B0\"\n",
    "                         u\"\\U000024C2-\\U0001F251\"\n",
    "                         \"]+\", flags=re.UNICODE)\n",
    "        text = emoji.sub(r'', text)\n",
    "        text = re.sub(r\"i'm\", \"i am\", text)\n",
    "        text = re.sub(r\"he's\", \"he is\", text)\n",
    "        text = re.sub(r\"she's\", \"she is\", text)\n",
    "        text = re.sub(r\"that's\", \"that is\", text)        \n",
    "        text = re.sub(r\"what's\", \"what is\", text)\n",
    "        text = re.sub(r\"where's\", \"where is\", text) \n",
    "        text = re.sub(r\"\\'ll\", \" will\", text)  \n",
    "        text = re.sub(r\"\\'ve\", \" have\", text)  \n",
    "        text = re.sub(r\"\\'re\", \" are\", text)\n",
    "        text = re.sub(r\"\\'d\", \" would\", text)\n",
    "        text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "        text = re.sub(r\"won't\", \"will not\", text)\n",
    "        text = re.sub(r\"don't\", \"do not\", text)\n",
    "        text = re.sub(r\"did't\", \"did not\", text)\n",
    "        text = re.sub(r\"can't\", \"can not\", text)\n",
    "        text = re.sub(r\"it's\", \"it is\", text)\n",
    "        text = re.sub(r\"couldn't\", \"could not\", text)\n",
    "        text = re.sub(r\"have't\", \"have not\", text)\n",
    "        text = re.sub(r\"[,.\\\"!@#$%^&*(){}?/;`~:<>+=-]\", \"\", text)\n",
    "        tokens = word_tokenize(text)\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        stop_words.discard(\"not\")\n",
    "        PS = PorterStemmer()\n",
    "        words = [PS.stem(w) for w in words if not w in stop_words]\n",
    "        words = ' '.join(words)\n",
    "        all_text.append(words)\n",
    "    return all_text\n",
    "\n",
    "all_text = clean_text(data)\n",
    "all_text[0:30]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
